#ENV VARS
export PROJECT_ID=kubernetes-demo-486609
export ZONE=europe-west3-a
export CLUSTER_NAME=hands-on

# 1. Check cluster 
gcloud container node-pools describe default-pool \
  --cluster "${CLUSTER_NAME}" \
  --zone "${ZONE}" \
  --project "${PROJECT_ID}" \
  --format="value(autoscaling.enabled,autoscaling.minNodeCount,autoscaling.maxNodeCount)"

# 2. Deploy
kubectl apply -f 30-deploy-nginx-hpa.yaml
kubectl rollout status -n hands-on deploy/hello  
kubectl get pods -n hands-on -o wide  

# 3. Browser Demo
kubectl port-forward -n hands-on svc/hello 8080:80
http://localhost:8080
http://localhost:8080/whoami

# 4. Echte LB-Demo (im Cluster)
kubectl run -n hands-on busy --image=busybox:1.36 --restart=Never -- \
  sh -c 'for i in $(seq 1 20); do wget -q -O- http://hello/whoami; done'
kubectl logs -n hands-on pod/busy
kubectl delete pod -n hands-on busy

# 5. ConfigMap Injection
kubectl exec -n hands-on <pod> -- sh -c 'echo $MY_MESSAGE'

# 6. Readiness Probe Demo (Traffic stoppt)
kubectl get endpoints -n hands-on hello -w

# 6.1 Readiness kaputt machen:
kubectl exec -n hands-on <pod> -- rm -f /tmp/ready

# 6.2 Zeigen, dass kein Traffic mehr auf diesen Pod geht: siehe 4.

# 6.3 Restore:
kubectl exec -n hands-on <pod> -- touch /tmp/ready

# 7. Liveness Probe Demo (Self-Healing)
kubectl get pods -n hands-on -w
# 7.1 Liveness kaputt machen:
kubectl exec -n hands-on <pod> -- rm -f /tmp/healthy

# 8. ReplicaSet Self-Heal
kubectl delete pod -n hands-on <pod>
kubectl get pods -n hands-on -w

# 9 Scaler
# 9.1 Beobachten (3 Terminals)
kubectl get hpa -n hands-on -w
kubectl get pods -n hands-on -w
kubectl get nodes -w

# 9.2 Load erzeugen (CPU Burn einschalten (triggert HPA))
POD=$(kubectl get pod -n hands-on -l app=hello -o jsonpath='{.items[0].metadata.name}')
echo "$POD"
kubectl exec -n hands-on "$POD" -c nginx -- sh -c 'touch /shared/burn'

# Ueber neue pods:
for p in $(kubectl get pods -n hands-on -l app=hello -o name); do
  kubectl exec -n hands-on "$p" -c nginx -- sh -c 'touch /shared/burn'
done